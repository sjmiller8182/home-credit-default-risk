{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mel Schwan, Stuart Miller, Justin Howard, Paul Adams\n",
    "# Lab Three: Clustering, Association Rules, or Recommenders\n",
    "## Capstone: Association Rule Mining, Clustering, or Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab3 Project Requirments -\n",
    "1. [Business understanding](#Businessunderstanding)\n",
    "    1. [Describe the purpose of the data set you selected](#Assessthecurrentsituation)\n",
    "    2. [Describe how you would define and measure the outcomes from the dataset](#CostBenefit)\n",
    "    3. [How would you measure the effectiveness of a good prediction algorithm](#Desiredoutputs)\n",
    "    4. [Why does your chosen validation method make sense for this specific dataset and the stakeholders needs?](#ValMethod)\n",
    "  \n",
    "2. [Data Understanding](#Dataunderstanding)\n",
    "    1. [Describe the meaning and type of data for each attribute in the data file](#Describedata)\n",
    "    2. [Verify data quality: Explain any missing values, duplicate data, and outliers](#Datareport)\n",
    "    3. [Give simple, appropriate statistics (range, mode, mean, median, variance, counts, etc.) for the most important attributes and describe what they mean](#Stats)\n",
    "    4. [Visualize the most important attributes appropriately (at least 5 attributes)](#Visualize)\n",
    "    5. [Explore relationships between attributes: Look at the attributes via scatter plots, correlation, cross-tabulation, group-wise averages, etc. as appropriate](#Correlations)\n",
    "    6. [Identify and explain interesting relationships between features and the class you are trying to predict](#relationships)\n",
    "    7. [Are there other features that could be added to the data or created from existing features? Which ones?](#Featurecreation)\n",
    "    8. [Outlier Removal](#OutlierRemoval)\n",
    "3. [Modeling and Evaluation](#Model)\n",
    "    1. [Option A: Cluster Analysis](#Cluster)\n",
    "    2. [Option B: Association Rule Mining](#Rule_mining)\n",
    "    2. [Option C: Collaborative Filtering](#Collaborative)\n",
    "4. [Deployment](#Deployment)\n",
    "    1. [How useful is your model for interested parties? ](#Useful)\n",
    "    2. [How would you measure the model's value if it was used by these parties?](#Value)\n",
    "    3. [How would your deploy your model for interested parties?](#Deploy)\n",
    "    4. [How often would your model need to be updated?](#Update)\n",
    "    5. [What other data should be collected? ](#Collect)\n",
    "\n",
    "A1. [Model Hyperparameter Tuning Details](#A2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our project will follow a hybrid methodology, mixing the expectations of the grading rubric with the CRISP_DM framework. CRISP-DM stands for the cross-industry process for data mining, which provides a structured approach to planning a data mining project. It is a robust and well-proven methodology.\n",
    "\n",
    "In the final assignment for this course, you will be using one of three different analysis methods:\n",
    "* Option A: Use transaction data for mining associations rules\n",
    "* Option B: Use clustering on an unlabeled dataset to provide insight or features\n",
    "* Option C: Use collaborative filtering to build a custom recommendation system\n",
    "Your choice of dataset will largely determine the task that you are trying to achieve. Though the\n",
    "dataset does not need to change from your previous tasks. For example, you might choose to use\n",
    "clustering on your data as a preprocessing step that extracts different features. Then you can use\n",
    "those features to build a classifier and analyze its performance in terms of accuracy (precision,\n",
    "recall) and speed. Alternatively, you might choose a completely different dataset and perform rule\n",
    "mining or build a recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../_images/crisps-dm.png\" style=\"width:550px;height:450px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Stage One - Business understanding  <a class=\"anchor\" id=\"Businessunderstanding\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Describe the purpose of the data set you selected (Q1A)<a class=\"anchor\" id=\"Assessthecurrentsituation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. Requirements - <a class=\"anchor\" id=\"Requirements\"></a> \n",
    "-  Requirements\n",
    "  -  Perform a descriptive analysis of a data set: exploring the statistical summaries of the features, visualizing important relationships among the attributes, and making conclusions from the visualizations and analysis. The analytical framework we will follow is a combination of the CRISP-DM framework and the specific requirements laid out within the Machine Learning Lab 1: Visualization.\n",
    "  -  Demonstrate the team's understanding of ? to best predict ?.\n",
    "  \n",
    "- Constraints\n",
    "  -  Time :  \n",
    "  -  Data :  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3.Risks and contingencies <a class=\"anchor\" id=\"Risks\"></a>\n",
    "  -  Outlier assumptions : \n",
    "  \n",
    "  -  Missing data assumptions : \n",
    "      -  \n",
    "      - \n",
    "      - \n",
    "      \n",
    "  -  Joined dataset assumptions \n",
    "\n",
    "  - Elimintation of features \n",
    "       - Elimintation of features can eliminate key factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4.Terminology <a class=\"anchor\" id=\"Terminology\"></a>\n",
    "Subject Matter Terminology:\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.5.Costs and benefits (Q1B) <a class=\"anchor\" id=\"CostBenefit\"></a>\n",
    "- Cost\n",
    "    -  \n",
    "\n",
    "- Benefits\n",
    "  -  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.2 Describe how you would define and measure the outcomes from the dataset (Q1B) <a class=\"anchor\" id=\"CostBenefit\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Business success criteria**\n",
    "- Produce a model capable of predicting \n",
    "- Build model capable of predicting whether an \n",
    "- Reduce \n",
    "\n",
    "**Data mining success criteria**\n",
    "- Build a data model that has a ROC value of .80 - .90    \n",
    "- Define successful ?\n",
    "    - \n",
    "    - \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.3 Describe how you would define and measure the outcomes from the dataset? (Q1C) <a class=\"anchor\" id=\"Desiredoutputs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our analytic goal is to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.4 Why does your chosen validation method make sense for this specific dataset and the stakeholders needs? (Q1D) <a class=\"anchor\" id=\"ValMethod\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Stage  Two - Data Understanding <a class=\"anchor\" id=\"Dataunderstanding\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Describe the meaning and type of data for each attribute in the data file (Q2A) <a class=\"anchor\" id=\"Describedata\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data comes from the \n",
    "Our analysis features the use of several Python libraries, such as Pandas, in addition to a custom data cleaning script foinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Verify data quality: Explain any missing values, duplicate data, and outliers (Q2B) <a class=\"anchor\" id=\"Datareport\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Give simple, appropriate statistics (range, mode, mean, median, variance, counts, etc.) for the most important attributes and describe what they mean (Q2C) <a class=\"anchor\" id=\"Stats\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. Missing Data <a class=\"anchor\" id=\"MissingData\"></a>\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. Outliers <a class=\"anchor\" id=\"Outliers\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High Values in "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning Script\n",
    "\n",
    "All the cleaning discussed in the sections above are implemented in `cleaning.py`.\n",
    "This script contains a function (`read_clean_data`) to apply the cleaning steps and return the cleaned dataset for work.\n",
    "\n",
    "**Details**  \n",
    "* Cleaning\n",
    "  * Read csv with Pandas (setting correct data types)\n",
    "  * Drop columns that will not be used\n",
    "  * Recode NA values that are not listed as np.nan\n",
    "  * Formattings\n",
    "  * Encode categorical variables\n",
    "* Returns\n",
    "  * DataFrame with cleaned data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Visualize the most important attributes appropriately (Q2D) <a class=\"anchor\" id=\"Visualize\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "info\n",
    "\n",
    "-\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Distributions  <a class=\"anchor\" id=\"Distributions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the exploration focused on the use of box plots and histograms for visualing continuous variables and bar charts for visualizing categorical variables. These graphical formats permit the easy identification of skewedness and help us identify outliers. \n",
    "\n",
    "Variables we expect to be important were selected for univariate visualization, such as `AMT_INCOME_TOTAL` and `AMT_ANNUITY`. The histogram shows the features of the main distribution, while the behavior of the tails and extreme values are shown in the boxplots. The distribution of incomes is extremely long-tailed and right-skewed, as can be expected with most any income distribution. \n",
    "\n",
    "Boxplots indicate a large number of outliers in both the `AMT_TOTAL_INCOME` and `AMT_ANNUITY` feature and one extreme outlier that will require closer exmaination. A comparison of each distribution in its raw and transformed form is displayed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Explore relationships between attributes: Look at the attributes via scatter plots, correlation, cross-tabulation, group-wise averages, etc. as appropriate (Q2E) <a class=\"anchor\" id=\"Correlations\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Identify and explain interesting relationships between features and the class you are trying to predict (Q2F) <a class=\"anchor\" id=\"relationships\"></a>\n",
    "\n",
    "infor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Are there other features that could be added to the data or created from existing features (Q2G) <a class=\"anchor\" id=\"Featurecreation\"></a>\n",
    "Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Outlier Removal (Q2H)<a class=\"anchor\" id=\"OutlierRemoval\"></a>\n",
    "#### Feature Selection for Outlier Removal\n",
    "Outlier removal was performed following feature analysis, imputation, and engineering to allow for a complete assessment of the raw data. The outlier\n",
    "removal process required to first identify the order of feature importance. To perform this, we applied a Random Forest and used the resutling Gini \n",
    "coefficients to identify importance. Categorical features were one-hot encoded prior to constructing a random forest. While there may be some potential minimization of categorical importance from this approach (through introducing more parameters and slightly dilluting the correlation), any residual collinearity following variable elimination would be likewise minimized by further separating significance of categorical variables in preference to other collinear, continuous and nominally scaled integer features.\n",
    "\n",
    "Using the random forest output, the top 15 features were identified. We decided to analyze 15 features for outlier removal due to the exponentially decaying significance of subsequent features to the target and the negative impact of removing too many observations. We performed analysis on these 15 features using histograms to assess dispersion and measured skewness on all transformed features to identify the most practical transformations (root, logarithmic, squared) required for normalization. Skewness around +/-1 was considered extreme, between +/-0.5 as moderate, and 0 as non-existent. Of these 15 features analyzed, eight features had outliers removed; highly imputed features and those with \n",
    "\n",
    "#### Three Standard Deviations\n",
    "Once normalized, features were then stripped of outliers. The outlier treatment applied uses z-tests to assess statistical dispersion of the data. Values beyond +/- three standard deviations are dropped from the model. This approach was selected because three standard deviations covers roughly 99.78% of all data, eliminating only extreme outliers. Three standard deviations is also within a reliable range for hypothesis testing. After dropping outlying observations from the data, we then back-transformed it to allow prior knowledge in the data to remain for its use in classification modeling.\n",
    "\n",
    "#### Quantifying Outlier Removal Impact\n",
    "Prior to outlier removal, the dataset had 307,511 observations. Following removal, the volume decreased by 21.399 percent to a count of 241,744 observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Stage  Three - Modeling and Evaluation <a class=\"anchor\" id=\"Model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Option A: Cluster Analysis (Q3A)<a class=\"anchor\" id=\"Cluster\"></a>\n",
    "\n",
    "Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Option B: Association Rule Mining (Q3B)<a class=\"anchor\" id=\"Rule_mining\"></a>\n",
    "\n",
    "Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Option C: Collaborative Filtering (Q3C)<a class=\"anchor\" id=\"Collaborative\"></a>\n",
    "\n",
    "Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Stage Five - Deployment (Q3) <a class=\"anchor\" id=\"Deployment\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Next Stage Deployment <a class=\"anchor\" id=\"Deployment\"></a>\n",
    "\n",
    "The next stage in the CRISP-DM is deployment. After model building and evaluation, we are ready to deploy our code representation of the model into a production environment and solve our original business problem.\n",
    "Our business problem is to give Home Credit loan evaluators access to a model that evaluates an applicantâ€™s current and past financial history in determining whether to approve the requested loan.\n",
    "\n",
    "#### How useful is your model for interested parties?<a class=\"anchor\" id=\"Useful\"></a>\n",
    "\n",
    "We believe this model would be useful for loan departments loan evaluators. This is contingent of discovering how some of the external variables are created. With this and some additional consistancy in top scoring features we could improve the accuracy\n",
    "\n",
    "#### How would you measure the model's value if it was used by these parties?<a class=\"anchor\" id=\"Value\"></a>\n",
    "\n",
    "This model should be tested in parrellel to present evaluation proceess. Then after a set period of time compare human evaluation to model based accuracy. If the results were the same, the minimum resulting savings would be the salaries of the loan evaluators. Added value would result from the acceleration of the loan approval process.\n",
    "\n",
    "#### How would your deploy your model for interested parties?<a class=\"anchor\" id=\"Deploy\"></a>\n",
    "\n",
    "Depending on the resources,  available, models can be deployed as batch or real-time predictions. Home Credits current process is a batch implementation. The applicant fills out the form which is then digitized and sent to the loan approval department. During the loan approval, the collected data will need to be cleaned and normalized before processed through the machine learning predictive model.\n",
    "\n",
    "#### How often would the model need to be updated?<a class=\"anchor\" id=\"Update\"></a>\n",
    "\n",
    "The metrics for the customer is continually updated. On a scheduled cycle, this new dataset is analyzed through the current model. Doing batch cycles will allow for consistency in whether an applicant is approved or not.  However, model designs will change as newer technology are available or changing business environments make reengineering required.\n",
    "\n",
    "#### What other data should be collected?<a class=\"anchor\" id=\"Collect\"></a>\n",
    "\n",
    "In addition to the clarification of data we have described above, there would be a tremendous value in the financials for the industry that the individual is working. These financials could assist in prediction of any future concerns for the applicants present income.\n",
    "\n",
    "Below is a typical example of a batch deployment \\[1\\].\n",
    "\n",
    "<img src=\"../_images/batchdeployment_process.png\" style=\"width:800px;height:375px\"/>\n",
    "\n",
    "\n",
    "\n",
    "## References\n",
    "\n",
    "\\[1\\] J. Kervizic, Overview of Different Approaches to Deploying Machine Learning Models in Production, June 2016.\n",
    "Accessed on: Feb. 15, 2019. \\[Online\\].\n",
    "Available: https://www.kdnuggets.com/2019/06/approaches-deploying-machine-learning-production.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A1. Parameter Tuning <a class=\"anchor\" id=\"A2\"></a>\n",
    "\n",
    "The code and output of model tuneing in shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
